from rb_utils.database import sqldb
from fastapi import HTTPException, status
from db.models.rbac import RelationManagerUserMapping, Reportee, User, UserMetaData
from sqlalchemy.future import select

from schemas.admin_schema import BrokingClaimsUserRequest, BrokingOpsUserRequest, BrokingSalesUserRequest, \
    DealerUserRequest, HelpDeskUserRequest, ICUserRequest, OemUserRequest, WorkshopUserRequest
from utils.verification import get_hashed_password, autogenerated_password_reset


def remove_corrupt(func):
    """
    The remove_corrupt function is a decorator that wraps the get_corrupt function.
    It takes in a dataframe and returns the same dataframe with corrupt rows removed.
    The remove_corrupt function also returns a list of ids for all corrupt rows.

    :param func: Pass the function to be decorated
    :return: The dataframe and a list of corrupt ids
    """

    async def wrapper(*args, **kwargs):
        _, corrupt_ids = await func(*args, **kwargs)
        if corrupt_ids:
            kwargs.get("df").drop(corrupt_ids, inplace=True)
        return _, corrupt_ids

    return wrapper


class UserServices:

    @classmethod
    async def validate_df(cls, group_type, df, error_records):
        schema_dict = {
            "insurer": ICUserRequest,
            "dealer": DealerUserRequest,
            "broking_sales": BrokingSalesUserRequest,
            "workshop": WorkshopUserRequest,
            "oem": OemUserRequest,
            "broking_claims": BrokingClaimsUserRequest,
            "broking_ops": BrokingOpsUserRequest,
            "helpdesk": HelpDeskUserRequest
        }
        request_schema = schema_dict.get(group_type)
        corrupt_index = []
        for i in df.index:
            try:
                row = df.loc[i, :].to_dict()
                request_schema(**row)
            except Exception as e:
                corrupt_index.append(i)
                error_records.update({
                    i: str(e)
                })
        df.drop(corrupt_index, inplace=True)
        return corrupt_index

    @classmethod
    async def get_table_headers(cls, group_type):
        user_headers = [
            "admin_role_id", "salutation", "first_name", "middle_name", "last_name", "designation",
            "email", "mobile_no", "landline_no",
        ]
        user_meta_headers = ["user_id", "dealer_code", "workshop_code", ]

        user_dict = {
            "dealer": [],
            "broking_sales": ["employee_code", "region"],
            "workshop": ["employee_code"],
            "oem": ["employee_code"],
            "broking_claims": ["employee_code"],
            "broking_ops": [],
            "helpdesk": [],
            "insurer": ["address"]
        }

        user_meta_dict = {
            "dealer": [],
            "broking_sales": [],
            "workshop": [],
            "oem": ["oem_code"],
            "broking_claims": [],
            "broking_ops": [],
            "helpdesk": ["insurer_code"],
            "insurer": ["insurer_code"]
        }

        relationship_manager_dict = {
            "dealer": ["user_id", "relationship_manager_email", ],
            "broking_sales": [],
            "workshop": ["user_id", "relationship_manager_email", ],
            "oem": ["user_id", "relationship_manager_email"],
            "broking_claims": [],
            "broking_ops": [],
            "helpdesk": [],
            "insurer": ["user_id", "relationship_manager_email"]
        }

        reporting_dict = {
            "dealer": [],
            "broking_sales": ["reportee_emails"],
            "workshop": [],
            "oem": [],
            "broking_claims": [],
            "broking_ops": ["reporting_manager_email", "user_id"],
            "helpdesk": ["reporting_manager_email", "user_id"],
            "insurer": []
        }

        user_headers += (user_dict.get(group_type) or [])
        user_meta_headers += (user_meta_dict.get(group_type) or [])
        relationship_manager_headers = (relationship_manager_dict.get(group_type) or [])
        reporting_headers = (reporting_dict.get(group_type) or [])

        return user_headers, user_meta_headers, relationship_manager_headers, reporting_headers

    @classmethod
    @remove_corrupt
    async def get_id_by_email(cls, model_df, df, session, type=None, error_records=None):
        id_list = []
        corrupt_id_list = []
        email_series = model_df.relationship_manager_email if type == "RELATIONSHIP MANAGER" else model_df.reporting_manager_email
        data_model = RelationManagerUserMapping if type == "RELATIONSHIP MANAGER" else Reportee
        for index in email_series.index:
            try:
                manager_id = (await session.execute(select(User.id).where(User.email == email_series[index]).order_by(
                    User.created_at.desc()))).scalars().first()
                id_list.append(manager_id)
                if manager_id is None:
                    raise Exception(f"USER DOES NOT EXIST FOR EMAIL {email_series[index]}")

                user = (await session.execute(select(data_model.id).where(
                    data_model.user_id == df.loc[index, "user_id"]))).scalars().first()
                if user:
                    raise Exception(f"{type} FOR USER {df.loc[index, 'user_email']} ALREADY EXISTS")

            except Exception as e:
                corrupt_id_list.append(index)
                if isinstance(error_records, dict):
                    error_records.update({index: str(e)})
        return id_list, corrupt_id_list

    @classmethod
    async def get_id_by_email_list(cls, email_list):
        email_dict = {email: None for email in email_list}
        email_data = (await sqldb.execute(select(User.email, User.id).where(
            User.email.in_(email_list)).order_by(User.created_at.desc()))).all()
        if email_data:
            email_dict.update(email_data)

        return email_dict

    @classmethod
    async def bulk_upload_df(cls, data_model, data, session, data_type=None):
        try:
            await session.execute(data_model.__table__.insert(), data)
        except Exception as e:
            await session.rollback()
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"ERROR IN CREATING {data_type} ENTRIES, AS: {str(e)}"
            )

    @classmethod
    @remove_corrupt
    async def user_bulk_create(cls,background_tasks, df, user_headers, error_records, session):
        user_df = df[user_headers]
        user_df.loc[:, "admin_role_id"] = user_df.loc[:, "admin_role_id"].apply(lambda x: int(x))
        user_df["hashed_password"] = "test_password"
        corrupt_rows = []

        for i in user_df.index:
            try:
                user_data = user_df.loc[i, :].dropna().to_dict()
                user = (await session.execute(select(User).where(
                    User.email == user_data.get("email")).order_by(User.created_at.desc()))).scalars().first()
                if user:
                    await user.update(key=user.id, **user_data)
                else:
                    password = f"{user_data['first_name']}{user_data['mobile_no'][0:5]}"
                    user_data['hashed_passoword'] = get_hashed_password(password=password)
                    user = await User.create(**user_data)
                    await autogenerated_password_reset(background_tasks=background_tasks, old_password=password,
                                                       email=user_data['email'])
                df.loc[i, "user_id"] = user.id
                df.loc[i, "user_email"] = user.email
            except Exception as e:
                corrupt_rows.append(i)
                error_records.update({i: str(e)})
        return "_", corrupt_rows

    @classmethod
    async def user_meta_bulk_create(cls, df, user_meta_headers, session):
        user_meta_df = df[user_meta_headers]
        await cls.bulk_upload_df(data=user_meta_df.to_dict(orient="records"), data_model=UserMetaData, \
                                 session=session, data_type="USER META DATA")

    @classmethod
    async def rm_bulk_create(cls, df, relationship_manager_headers, error_records, session):

        rm_df = df[relationship_manager_headers]
        rm_df["relationship_manager_id"], corrupt_id_list = await cls.get_id_by_email(model_df=rm_df, df=df,
                                                                                      session=session, \
                                                                                      error_records=error_records,
                                                                                      type="RELATIONSHIP MANAGER")
        rm_df.drop(columns=["relationship_manager_email"], inplace=True)
        rm_df.drop(corrupt_id_list, inplace=True)
        await UserServices.bulk_upload_df(data=rm_df.to_dict(orient="records"), \
                                          data_model=RelationManagerUserMapping, session=session,
                                          data_type="RELATIONSHIP MANAGER")

    @classmethod
    async def reportee_bulk_upload(cls, df, reporting_df, error_records, session, reportee_push_data):
        for i in reporting_df.index:
            if not reporting_df.loc[i, "reportee_emails"]:
                continue

            email_list = [i.strip() for i in reporting_df.loc[i, "reportee_emails"].split("|")]
            reportee_data = await cls.get_id_by_email_list(email_list=email_list)

            reportee_ids = list(reportee_data.values())
            existing_reportees = (await session.execute(select(Reportee.user_id).where(
                Reportee.user_id.in_(list(reportee_ids))))).scalars().all()

            if existing_reportees:
                error_records.update(
                    {i: f"REPORTING MANAGER FOR EMAILS {[email for _, email in reportee_data.items()]} ALREADY EXIST "})
                continue

            user_id = df.loc[i, "user_id"]
            wrong_emails = []
            for email, index in reportee_data.items():
                if index is None:
                    wrong_emails.append(email)
                    continue

                reportee_push_data.append({
                    "user_id": index,
                    "manager_id": user_id,
                    "is_active": True
                })

            if wrong_emails:
                error_records.update({
                    i: f"USERS WITH EMAILS: {wrong_emails} DOES NOT EXIST, KINDLY CHECK ALL EMAILS."
                })
        await UserServices.bulk_upload_df(data=reportee_push_data, data_model=Reportee, session=session,
                                          data_type="REPORTEES")

    @classmethod
    async def reporting_manager(cls, df, reporting_df, error_records, session):
        reporting_df["manager_id"], corrupt_ids = await UserServices.get_id_by_email(
            df=df, model_df=reporting_df, session=session, error_records=error_records)
        reporting_df["is_active"] = True
        reporting_df.drop(corrupt_ids, inplace=True)
        reporting_df.drop(columns=["reporting_manager_email"], inplace=True)
        await UserServices.bulk_upload_df(data=reporting_df.to_dict(orient="records"), data_model=Reportee,
                                          session=session, data_type="REPORTING MANAGER")

    @classmethod
    async def reporting_bulk_create(cls, df, reporting_headers, group_type, error_records, session):
        reporting_df = df[reporting_headers]
        if group_type == "broking_sales":
            reportee_push_data = []
            await cls.reportee_bulk_upload(df, reporting_df, error_records, session, reportee_push_data)
        else:
            await cls.reporting_manager(df, reporting_df, error_records, session)
